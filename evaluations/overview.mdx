---
title: "Evaluations Overview"
description: "Test and benchmark your AI agents with deterministic assertions"
---

## What Are Evaluations?

Evaluations let you verify that your AI agent performed the expected actions. Instead of checking if the agent "said the right thing", you check if it **did the right thing** by examining database state changes.

## The Evaluation Flow

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TEST DEFINITION                              │
│                                                                      │
│  {                                                                   │
│    "prompt": "Post 'Hello' to #general",                            │
│    "assertions": [{                                                  │
│      "diff_type": "added",                                          │
│      "entity": "messages",                                          │
│      "where": { "text": { "contains": "Hello" } }                   │
│    }]                                                                │
│  }                                                                   │
└────────────────────────────────────┬────────────────────────────────┘
                                     │
                                     ▼
┌─────────────────────────────────────────────────────────────────────┐
│                         AGENT EXECUTION                              │
│                                                                      │
│  1. Environment created from template                                │
│  2. "Before" snapshot taken                                          │
│  3. Agent runs with prompt                                           │
│  4. "After" snapshot taken                                           │
│  5. Diff computed                                                    │
│                                                                      │
└────────────────────────────────────┬────────────────────────────────┘
                                     │
                                     ▼
┌─────────────────────────────────────────────────────────────────────┐
│                         ASSERTION EVALUATION                         │
│                                                                      │
│  diff: {                                                             │
│    inserts: [{ table: "messages", text: "Hello World!" }]           │
│  }                                                                   │
│                                                                      │
│  ✓ Assertion passed: Found 1 message containing "Hello"             │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

## Key Components

<CardGroup cols={2}>
  <Card title="Test Suites" icon="folder" href="/evaluations/test-suites">
    Collections of related tests
  </Card>
  <Card title="Assertions" icon="check-double" href="/evaluations/writing-assertions">
    Expected outcomes in JSON
  </Card>
  <Card title="DSL Reference" icon="code" href="/evaluations/dsl-reference">
    All operators and predicates
  </Card>
  <Card title="Benchmarks" icon="chart-line" href="/evaluations/benchmarks">
    Run evaluations at scale
  </Card>
</CardGroup>

## Quick Example

```python
from agent_diff import AgentDiff, PythonExecutorProxy, create_openai_tool
from agents import Agent, Runner

client = AgentDiff()

# Get a test suite
suites = client.list_test_suites(name="Slack Bench")
suite = client.get_test_suite(suites.testSuites[0].id, expand=True)

# Run a single test
test = suite.tests[0]
print(f"Running: {test.name}")
print(f"Prompt: {test.prompt}")

# Create environment for this test
env = client.init_env(testId=test.id)

# Start run (captures before state)
run = client.start_run(envId=env.environmentId, testId=test.id)

# Create and run agent
executor = PythonExecutorProxy(env.environmentId, client.base_url)
agent = Agent(
    name="Test Agent",
    tools=[create_openai_tool(executor)],
    instructions="Use execute_python to interact with Slack at https://slack.com/api/*"
)
await Runner.run(agent, test.prompt)

# Evaluate against assertions
result = client.evaluate_run(runId=run.runId)

print(f"\nResult: {'✓ PASSED' if result.passed else '✗ FAILED'}")
print(f"Score: {result.score}")
if result.failures:
    print(f"Failures: {result.failures}")

# Cleanup
client.delete_env(envId=env.environmentId)
```

## Built-in Benchmarks

Agent Diff includes pre-built benchmarks:

| Benchmark | Service | Tests | Description |
|-----------|---------|-------|-------------|
| Slack Bench | Slack | 20 | Message sending, channel ops, reactions |
| Linear Bench | Linear | 40 | Issue CRUD, labels, comments, workflow |

## Why State-Based Evaluation?

Traditional LLM evaluation checks if the output "looks right":
- Fuzzy matching
- Semantic similarity
- Human review

Agent Diff checks if the agent **actually did the right thing**:
- Deterministic: Same input → Same pass/fail
- Precise: Exact field-level assertions
- Comprehensive: Checks all side effects

```
Traditional:
  "Did the agent say it posted a message?" → Maybe ✓

Agent Diff:
  "Is there a new row in messages table with text='Hello'?" → Definite ✓ or ✗
```

## Next Steps

<CardGroup cols={2}>
  <Card title="Test Suites" icon="folder" href="/evaluations/test-suites">
    Learn how to organize tests
  </Card>
  <Card title="Writing Assertions" icon="pen" href="/evaluations/writing-assertions">
    Define expected outcomes
  </Card>
</CardGroup>

